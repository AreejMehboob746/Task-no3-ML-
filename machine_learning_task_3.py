# -*- coding: utf-8 -*-
"""Machine Learning task 3(Areej Mehboob).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOGB4oEZd2Hxjj4SYWaSyji2O82EGNeX
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV

# Load the dataset from a local file (ensure you have downloaded the dataset from Kaggle)
file_path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv'
data = pd.read_csv(file_path)

# Data Cleaning and Preprocessing
data = data.drop(columns=['customerID'])  # Dropping customerID as it is not needed
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data = data.dropna()  # Dropping rows with missing values

# Convert categorical columns to numerical
categorical_columns = data.select_dtypes(include=['object']).columns
data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

# Splitting data into features and target
X = data.drop(columns=['Churn_Yes'])
y = data['Churn_Yes']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model selection and training
# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Model evaluation
lr_predictions = lr_model.predict(X_test)
rf_predictions = rf_model.predict(X_test)

print("Logistic Regression Performance:")
print(confusion_matrix(y_test, lr_predictions))
print(classification_report(y_test, lr_predictions))
print("Accuracy:", accuracy_score(y_test, lr_predictions))

print("\nRandom Forest Performance:")
print(confusion_matrix(y_test, rf_predictions))
print(classification_report(y_test, rf_predictions))
print("Accuracy:", accuracy_score(y_test, rf_predictions))

# Fine-tuning using GridSearchCV for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# Evaluate the tuned model
best_rf_model = grid_search.best_estimator_
tuned_rf_predictions = best_rf_model.predict(X_test)

print("\nTuned Random Forest Performance:")
print(confusion_matrix(y_test, tuned_rf_predictions))
print(classification_report(y_test, tuned_rf_predictions))
print("Accuracy:", accuracy_score(y_test, tuned_rf_predictions))